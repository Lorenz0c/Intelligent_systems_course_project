{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweet_no_puntation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkoTFfE2eo4R"
      },
      "source": [
        "# Text without puntation, twittwer dataset."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6hQSLYnL-vf"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNzS66UTMAL7"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "#Download dataset.\n",
        "#(only a part of the train data of the original dataset have been taken in to consideration).\n",
        "\n",
        "Data_URL = \"http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\"\n",
        "\n",
        "file_path = tf.keras.utils.get_file(\"twitter.zip\", Data_URL, extract=True)\n",
        "\n",
        "csv_file = os.path.join(os.path.dirname(file_path), 'training.1600000.processed.noemoticon.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qswgm1-RVsbt"
      },
      "source": [
        "BATCH_SIZE=50\n",
        "COLUMN_NAMES=['label','id','date','query','user','text'] #the names of the column are specified in the web page that contains the dataset.\n",
        "LABEL_NAME='label'\n",
        "COLUMN_SELECTED=['label','text']\n",
        "\n",
        "#Only 5000 messages will be taken into consideration.\n",
        "DATASET_SIZE=16000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9YzkLlSGAoP"
      },
      "source": [
        "#Create the dataset.\n",
        "complete_dataset=tf.data.experimental.make_csv_dataset(csv_file,1,column_names=COLUMN_NAMES,label_name=LABEL_NAME,select_columns=COLUMN_SELECTED,num_epochs=1,shuffle=True,shuffle_buffer_size=1600000,shuffle_seed=12311)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JdJmrn-MEDO"
      },
      "source": [
        "#delete puntation.\n",
        "def insert_space(text):\n",
        "  text=tf.strings.regex_replace(text,\"([.,!?();])\",\"\")\n",
        "  #encode in UTF-8\n",
        "  text=tf.strings.unicode_decode(text,input_encoding=\"UTF-8\")\n",
        "  text=tf.strings.unicode_encode(text,output_encoding=\"UTF-8\")\n",
        "  return text\n",
        "\n",
        "# Convert label values 0-2-4 in 0-1-2:\n",
        "def convert_label(label):\n",
        "  return int(label/2)\n",
        "\n",
        "\n",
        "#The lines of the dataset can start with ham (if they are not spam) or with spam (if they are spam).\n",
        "#The label associated to spam is 1, while the one associated with ham is 0.  \n",
        "\n",
        "def process_line(line_text,label):\n",
        "    text = tf.stack(list(line_text.values()), axis=1)\n",
        "    text=insert_space(text[0][0])\n",
        "    converted_label=convert_label(label[0])\n",
        "    return text, converted_label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOfiCwxSL-qj"
      },
      "source": [
        "labeled_dataset=complete_dataset.map(process_line, num_parallel_calls=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnOaqG-Kk4Ud"
      },
      "source": [
        "#Take only 16000 example.\n",
        "shuffled_dataset=labeled_dataset.take(DATASET_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_-bq49nSNrT"
      },
      "source": [
        "#Create the vocabulary and store its dimension.\n",
        "tokenizer=tfds.features.text.Tokenizer()\n",
        "\n",
        "vocabulary=set()\n",
        "for text,_ in shuffled_dataset:\n",
        "  token=tokenizer.tokenize(text.numpy())\n",
        "  vocabulary.update(token)\n",
        "\n",
        "VOCABULARY_SIZE=len(vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUnB6i8dTuwb"
      },
      "source": [
        "#Define the encoder.\n",
        "encoder=tfds.features.text.TokenTextEncoder(vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lctdYjH-T2Wt"
      },
      "source": [
        "#Encode.\n",
        "def encode(text, label):\n",
        "  encoded_text = encoder.encode(text.numpy())\n",
        "  return encoded_text, label\n",
        "\n",
        "def encode_map_fn(text, label):\n",
        "  # py_func doesn't set the shape of the returned tensors.\n",
        "  encoded_text, label = tf.py_function(encode, \n",
        "                                       inp=[text, label], \n",
        "                                       Tout=(tf.int64, tf.int32))\n",
        "\n",
        "  # `tf.data.Datasets` work best if all components have a shape set\n",
        "  #  so set the shapes manually: \n",
        "  encoded_text.set_shape([None])\n",
        "  label.set_shape([])\n",
        "\n",
        "  return encoded_text, label\n",
        "\n",
        "encoded_dataset = shuffled_dataset.map(encode_map_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QLJlcwSUDvF"
      },
      "source": [
        "#define the number of element in the training and in the test set.\n",
        "BATCH_NUMBER=int(round(DATASET_SIZE/BATCH_SIZE))\n",
        "TEST_NUMBER=int(BATCH_NUMBER/4)\n",
        "\n",
        "# Divide the messages in the training and in the test sets.\n",
        "ds_train=encoded_dataset.take((BATCH_NUMBER-TEST_NUMBER)*BATCH_SIZE)\n",
        "ds_test=encoded_dataset.skip((BATCH_NUMBER-TEST_NUMBER)*BATCH_SIZE)\n",
        "\n",
        "# The strings must be batched and padded to the length of the longest string in the batch.\n",
        "ds_train=ds_train.padded_batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "ds_test=ds_test.padded_batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYRgGqZ5v1be"
      },
      "source": [
        "EMBEDDING_DIM=32\n",
        "\n",
        "#Define the model.\n",
        "model=keras.Sequential([\n",
        "                        keras.layers.Embedding(VOCABULARY_SIZE+2, EMBEDDING_DIM),\n",
        "                        keras.layers.Bidirectional(keras.layers.GRU(64,return_sequences=True)),\n",
        "                        keras.layers.Bidirectional(keras.layers.GRU(64,return_sequences=True)),\n",
        "                        keras.layers.Bidirectional(keras.layers.GRU(32)),\n",
        "                        keras.layers.Dense(32, activation='relu'),\n",
        "                        keras.layers.Dense(32, activation='relu'),\n",
        "                        keras.layers.Dropout(0.25),\n",
        "                        keras.layers.Dense(32, activation='relu'),\n",
        "                        keras.layers.Dropout(0.25),\n",
        "                        keras.layers.Dense(3),\n",
        "])\n",
        "\n",
        "#Compile the model.\n",
        "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])   \n",
        "\n",
        "#Train the model.\n",
        "history=model.fit(ds_train,validation_data=ds_test,epochs=10) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoUfQfUGwfmZ"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'], '')\n",
        "plt.xlabel(\"Epoche\")\n",
        "plt.ylabel('Numero assegnameti corretti normalizzato')\n",
        "plt.legend(['Training', 'Test'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgrwZ2gO4YSZ"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'], '')\n",
        "plt.xlabel(\"Epoche\")\n",
        "plt.ylabel('Errore calcolato tramite loss function')\n",
        "plt.legend(['Training', 'Test'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}