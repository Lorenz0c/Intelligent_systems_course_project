{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMS_lowercase.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dKNLEhUgmBP"
      },
      "source": [
        "# Text without uppercase characters, SMS Spam dataset."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoEe7E2VUGE7"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASVFsZDSUIMR"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "#Download dataset.\n",
        "#(only a part of the train data of the original dataset have been taken in to consideration).\n",
        "\n",
        "Data_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
        "\n",
        "file_path = tf.keras.utils.get_file(\"sms.zip\", Data_URL, extract=True)\n",
        "\n",
        "text_file = os.path.join(os.path.dirname(file_path), 'SMSSpamCollection')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJW_FSVydJ12"
      },
      "source": [
        "#insert space before puntation and replace uppercase character with lowercase.\n",
        "def lowercase(text):\n",
        "  text=tf.strings.regex_replace(text,\"([.,!?();])\",\" \\\\1 \")\n",
        "  text=tf.strings.regex_replace(text,\"  \",\" \")\n",
        "  text=tf.strings.regex_replace(text,\"\\. \\. \\. \\.\",\"....\")\n",
        "  text=tf.strings.regex_replace(text,\"\\. \\. \\.\",\"...\")\n",
        "  text=tf.strings.regex_replace(text,\"\\. \\.\",\"..\")\n",
        "  #replace uppercase charactesrs.\n",
        "  text=tf.strings.lower(text)\n",
        "  return text    \n",
        "\n",
        "#The lines of the dataset can start with ham (if they are not spam) or with spam (if they are spam).\n",
        "#The label associated to spam is 1, while the one associated with ham is 0.  \n",
        "def process_line_lowercase(line_text):\n",
        "  if tf.strings.regex_full_match(line_text,'ham.*'):\n",
        "    text=tf.strings.substr(line_text,4,tf.strings.length(line_text)-4)\n",
        "    text=lowercase(text) \n",
        "    return text, tf.constant(0)\n",
        "  text=tf.strings.substr(line_text,5,tf.strings.length(line_text)-5)\n",
        "  text=lowercase(text)  \n",
        "  return text, tf.constant(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZDzpKHgUVhj"
      },
      "source": [
        "#create the dataset (composed by the couples: text-lable associated).\n",
        "lines_dataset=tf.data.TextLineDataset(text_file)\n",
        "\n",
        "# dataset with the text in lowercase.\n",
        "labeled_dataset=lines_dataset.map(process_line_lowercase, num_parallel_calls=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9qHljwEiyNP"
      },
      "source": [
        "#Create the vocabulary and check its dimension.\n",
        "tokenizer=tfds.features.text.Tokenizer()\n",
        "\n",
        "vocabulary=set()\n",
        "for text,_ in labeled_dataset:\n",
        "  token=tokenizer.tokenize(text.numpy())\n",
        "  vocabulary.update(token)\n",
        "\n",
        "VOCABULARY_SIZE=len(vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOYrKkydFwZh"
      },
      "source": [
        "#Define the encoder.\n",
        "encoder=tfds.features.text.TokenTextEncoder(vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlQbW702GP16"
      },
      "source": [
        "#Encode.\n",
        "def encode(text, label):\n",
        "  encoded_text = encoder.encode(text.numpy())\n",
        "  return encoded_text, label\n",
        "\n",
        "def encode_map_fn(text, label):\n",
        "  # py_func doesn't set the shape of the returned tensors.\n",
        "  encoded_text, label = tf.py_function(encode, \n",
        "                                       inp=[text, label], \n",
        "                                       Tout=(tf.int64, tf.int32))\n",
        "\n",
        "  # `tf.data.Datasets` work best if all components have a shape set\n",
        "  #  so set the shapes manually: \n",
        "  encoded_text.set_shape([None])\n",
        "  label.set_shape([])\n",
        "\n",
        "  return encoded_text, label\n",
        "\n",
        "encoded_dataset = labeled_dataset.map(encode_map_fn)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JlxR0bgi5eo"
      },
      "source": [
        "n=0\n",
        "for i in encoded_dataset.as_numpy_iterator():\n",
        "  n=n+1\n",
        "\n",
        "#define the number of element in the training and in the test set.\n",
        "DATASET_SIZE=n #is equal to 5574: number indicated in the dataset description.\n",
        "BATCH_SIZE = 25\n",
        "BATCH_NUMBER=int(round(DATASET_SIZE/BATCH_SIZE))\n",
        "TEST_NUMBER=int(BATCH_NUMBER/4)\n",
        "\n",
        "# Shuffle the messages in the dataset, and divide them in the training and in the test sets.\n",
        "dataset_training_test=encoded_dataset.shuffle(DATASET_SIZE)\n",
        "ds_train=dataset_training_test.take((BATCH_NUMBER-TEST_NUMBER)*BATCH_SIZE)\n",
        "ds_test=dataset_training_test.skip((BATCH_NUMBER-TEST_NUMBER)*BATCH_SIZE)\n",
        "\n",
        "# The strings must be batched and padded to the length of the longest string in the batch.\n",
        "ds_train=ds_train.padded_batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "ds_test=ds_test.padded_batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SP0o4ZDOd2c"
      },
      "source": [
        "EMBEDDING_DIM=32\n",
        "\n",
        "#Define the model.\n",
        "model=keras.Sequential([\n",
        "                        keras.layers.Embedding(VOCABULARY_SIZE+1, EMBEDDING_DIM),\n",
        "                        keras.layers.Bidirectional(keras.layers.SimpleRNN(32,return_sequences=True)),\n",
        "                        keras.layers.Bidirectional(keras.layers.SimpleRNN(16)),\n",
        "                        keras.layers.Dense(16, activation='relu'),\n",
        "                        keras.layers.Dense(16, activation='relu'),\n",
        "                        keras.layers.Dropout(0.25),\n",
        "                        keras.layers.Dense(1),\n",
        "])\n",
        "\n",
        "#Compile the model.\n",
        "model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "#Train the model.\n",
        "history_original=model.fit(ds_train,validation_data=ds_test,epochs=15)           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5A0W0Q3yM2B"
      },
      "source": [
        "plt.plot(history_original.history['accuracy'])\n",
        "plt.plot(history_original.history['val_accuracy'], '')\n",
        "plt.xlabel(\"Epoche\")\n",
        "plt.ylabel('Numero assegnameti corretti normalizzato')\n",
        "plt.legend(['Training', 'Test'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWg3lWMRm8-v"
      },
      "source": [
        "plt.plot(history_original.history['loss'])\n",
        "plt.plot(history_original.history['val_loss'], '')\n",
        "plt.xlabel(\"Epoche\")\n",
        "plt.ylabel('Errore calcolato tramite loss function')\n",
        "plt.legend(['Training', 'Test'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}