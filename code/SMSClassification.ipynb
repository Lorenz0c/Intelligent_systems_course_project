{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMSClassification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoEe7E2VUGE7"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import io\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASVFsZDSUIMR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "07b09b64-9fb1-4e41-91cf-9d697491089e"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "#Download dataset.\n",
        "#(only a part of the train data of the original dataset have been taken in to consideration).\n",
        "\n",
        "Data_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
        "\n",
        "file_path = tf.keras.utils.get_file(\"sms.zip\", Data_URL, extract=True)\n",
        "\n",
        "text_file = os.path.join(os.path.dirname(file_path), 'SMSSpamCollection')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "204800/203415 [==============================] - 0s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJW_FSVydJ12"
      },
      "source": [
        "# The different networks have different type of text in input. \n",
        "# The following are the different function used to obtain the text in inputs.\n",
        "\n",
        "#insert space before puntation.\n",
        "def insert_space(text):\n",
        "  text=tf.strings.regex_replace(text,\"([.,!?();])\",\" \\\\1 \")\n",
        "  text=tf.strings.regex_replace(text,\"  \",\" \")\n",
        "  text=tf.strings.regex_replace(text,\"\\. \\. \\. \\.\",\"....\")\n",
        "  text=tf.strings.regex_replace(text,\"\\. \\. \\.\",\"...\")\n",
        "  text=tf.strings.regex_replace(text,\"\\. \\.\",\"..\")\n",
        "  return text\n",
        "\n",
        "#eliminate puntation(input second network).\n",
        "def delete_puntation(text):\n",
        "  text=tf.strings.regex_replace(text,\"([.,!?();])\",\"\")\n",
        "  return text\n",
        "\n",
        "#eliminate comma(input third network).\n",
        "def delete_comma(text):\n",
        "  text=tf.strings.regex_replace(text,\"([,])\",\"\")\n",
        "  text=insert_space(text)\n",
        "  return text\n",
        "\n",
        "#insert lowercase(input fourt network).\n",
        "def lowercase(text):\n",
        "  text=tf.strings.regex_replace(text,\"([.,!?();])\",\" \\\\1 \")\n",
        "  text=tf.strings.regex_replace(text,\"  \",\" \")\n",
        "  text=tf.strings.regex_replace(text,\"\\. \\. \\. \\.\",\"....\")\n",
        "  text=tf.strings.regex_replace(text,\"\\. \\. \\.\",\"...\")\n",
        "  text=tf.strings.regex_replace(text,\"\\. \\.\",\"..\")\n",
        "  text=tf.strings.regex_replace(text,\"(\\w)\",\"\\L$1\")\n",
        "  return text     \n",
        "\n",
        "# Functions called to create the datsets------------------------------------------------\n",
        "\n",
        "#The lines of the dataset can start with ham (if they are not spam) or with spam (if they are spam).\n",
        "#The label associated to spam is 1, while the one associated with ham is 0.  \n",
        "\n",
        "def process_line(line_text):\n",
        "  if tf.strings.regex_full_match(line_text,'ham.*'):\n",
        "    text=tf.strings.substr(line_text,4,tf.strings.length(line_text)-4)\n",
        "    text=insert_space(text) \n",
        "    return text, tf.constant(0)\n",
        "  text=tf.strings.substr(line_text,5,tf.strings.length(line_text)-5)\n",
        "  text=insert_space(text)  \n",
        "  return text, tf.constant(1)\n",
        "\n",
        "def process_line_no_puntation(line_text):\n",
        "  if tf.strings.regex_full_match(line_text,'ham.*'):\n",
        "    text=tf.strings.substr(line_text,4,tf.strings.length(line_text)-4)\n",
        "    text=delete_puntation(text) \n",
        "    return text, tf.constant(0)\n",
        "  text=tf.strings.substr(line_text,5,tf.strings.length(line_text)-5)\n",
        "  text=delete_puntation(text)  \n",
        "  return text, tf.constant(1)\n",
        "\n",
        "def process_line_no_comma(line_text):\n",
        "  if tf.strings.regex_full_match(line_text,'ham.*'):\n",
        "    text=tf.strings.substr(line_text,4,tf.strings.length(line_text)-4)\n",
        "    text=delete_comma(text) \n",
        "    return text, tf.constant(0)\n",
        "  text=tf.strings.substr(line_text,5,tf.strings.length(line_text)-5)\n",
        "  text=delete_comma(text)  \n",
        "  return text, tf.constant(1)\n",
        "\n",
        "def process_line_lowercase(line_text):\n",
        "  if tf.strings.regex_full_match(line_text,'ham.*'):\n",
        "    text=tf.strings.substr(line_text,4,tf.strings.length(line_text)-4)\n",
        "    text=lowercase(text) \n",
        "    return text, tf.constant(0)\n",
        "  text=tf.strings.substr(line_text,5,tf.strings.length(line_text)-5)\n",
        "  text=lowercase(text)  \n",
        "  return text, tf.constant(1)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZDzpKHgUVhj"
      },
      "source": [
        "lines = io.open(text_file, encoding='UTF-8').read().split('\\n')\n",
        "\n",
        "#create the datasets (composed by the couples: text-lable associated).\n",
        "lines_dataset=tf.data.TextLineDataset(text_file)\n",
        "\n",
        "# first dataset (original text).\n",
        "labeled_dataset=lines_dataset.map(process_line, num_parallel_calls=AUTOTUNE)\n",
        "# dataset with the text without puntation.\n",
        "labeled_dataset_no_puntation=lines_dataset.map(process_line_no_puntation, num_parallel_calls=AUTOTUNE)\n",
        "# dataset with the text without comma.\n",
        "labeled_dataset_no_comma=lines_dataset.map(process_line_no_comma, num_parallel_calls=AUTOTUNE)\n",
        "# dataset with the text in lowercase.\n",
        "labeled_dataset_lowercase=lines_dataset.map(process_line_lowercase, num_parallel_calls=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9qHljwEiyNP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8470240f-3f5f-4da2-d6b6-892082b6ec4e"
      },
      "source": [
        "#Create the vocabulary and check its dimension (original dataset).\n",
        "tokenizer=tfds.features.text.Tokenizer()\n",
        "\n",
        "vocabulary=set()\n",
        "for text,_ in labeled_dataset:\n",
        "  token=tokenizer.tokenize(text.numpy())\n",
        "  vocabulary.update(token)\n",
        "\n",
        "VOCABULARY_SIZE=len(vocabulary)\n",
        "\n",
        "VOCABULARY_SIZE\n",
        "\n",
        "vocabulary=set()\n",
        "for text,_ in labeled_dataset_lowercase:\n",
        "  token=tokenizer.tokenize(text.numpy())\n",
        "  vocabulary.update(token)\n",
        "\n",
        "VOCABULARY_SIZE=len(vocabulary)\n",
        "\n",
        "VOCABULARY_SIZE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOYrKkydFwZh"
      },
      "source": [
        "#Define the encoder.\n",
        "encoder=tfds.features.text.TokenTextEncoder(vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlQbW702GP16"
      },
      "source": [
        "#Encode.\n",
        "def encode(text, label):\n",
        "  encoded_text = encoder.encode(text.numpy())\n",
        "  return encoded_text, label\n",
        "\n",
        "def encode_map_fn(text, label):\n",
        "  # py_func doesn't set the shape of the returned tensors.\n",
        "  encoded_text, label = tf.py_function(encode, \n",
        "                                       inp=[text, label], \n",
        "                                       Tout=(tf.int64, tf.int32))\n",
        "\n",
        "  # `tf.data.Datasets` work best if all components have a shape set\n",
        "  #  so set the shapes manually: \n",
        "  encoded_text.set_shape([None])\n",
        "  label.set_shape([])\n",
        "\n",
        "  return encoded_text, label\n",
        "\n",
        "encoded_dataset = labeled_dataset.map(encode_map_fn)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JlxR0bgi5eo"
      },
      "source": [
        "n=0\n",
        "for i in encoded_dataset.as_numpy_iterator():\n",
        "  n=n+1\n",
        "\n",
        "#define the number of element in the training and in the test set.\n",
        "DATASET_SIZE=n #is equal to 5574: number indicated in the dataset description.\n",
        "BATCH_SIZE = 25\n",
        "BATCH_NUMBER=int(round(DATASET_SIZE/BATCH_SIZE))\n",
        "TEST_NUMBER=int(BATCH_NUMBER/4)\n",
        "\n",
        "# Shuffle the messages in the dataset, and divide them in the training and in the test sets.\n",
        "dataset_training_test=encoded_dataset.shuffle(DATASET_SIZE)\n",
        "ds_train=dataset_training_test.take((BATCH_NUMBER-TEST_NUMBER)*BATCH_SIZE)\n",
        "ds_test=dataset_training_test.skip((BATCH_NUMBER-TEST_NUMBER)*BATCH_SIZE)\n",
        "\n",
        "# The strings must be batched and padded to the length of the longest string in the batch.\n",
        "ds_train=ds_train.padded_batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "ds_test=ds_test.padded_batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SP0o4ZDOd2c"
      },
      "source": [
        "EMBEDDING_DIM=32\n",
        "\n",
        "#Define the model.\n",
        "model=keras.Sequential([\n",
        "                        keras.layers.Embedding(VOCABULARY_SIZE+1, EMBEDDING_DIM),\n",
        "                        keras.layers.Bidirectional(keras.layers.SimpleRNN(32,return_sequences=True)),\n",
        "                        keras.layers.Bidirectional(keras.layers.SimpleRNN(16)),\n",
        "                        tf.keras.layers.Dense(16, activation='relu'),\n",
        "                        tf.keras.layers.Dense(16, activation='relu'),\n",
        "                        tf.keras.layers.Dropout(0.25),\n",
        "                        tf.keras.layers.Dense(1),\n",
        "])\n",
        "\n",
        "#Compile the model.\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvbniZnr_0E6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "0a71c29d-f6fb-4d15-940a-1c67786614e6"
      },
      "source": [
        "#Train the model.\n",
        "history_original=model.fit(ds_train,validation_data=ds_test,epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "     16/Unknown - 2s 112ms/step - loss: 0.4817 - accuracy: 0.8600"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-aff5ed63692b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Train the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory_original\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}